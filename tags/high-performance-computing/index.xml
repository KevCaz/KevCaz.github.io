<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>High Performance Computing on KevCaz's Website</title><link>https://kevcaz.github.io/tags/high-performance-computing/</link><description>Recent content in High Performance Computing on KevCaz's Website</description><generator>Hugo</generator><language>en-us</language><copyright>Content under CC0 1.0 Universal unless otherwise specified.</copyright><lastBuildDate>Thu, 20 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://kevcaz.github.io/tags/high-performance-computing/index.xml" rel="self" type="application/rss+xml"/><item><title>Running parallel simulations with scripts that take two or more varying parameters</title><link>https://kevcaz.github.io/notes/2020/08/2020-08-20_running_parallel_simulations/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://kevcaz.github.io/notes/2020/08/2020-08-20_running_parallel_simulations/</guid><description>&lt;p>Another note about how I work with &lt;a href="https://slurm.schedmd.com">Slurm&lt;/a>! Today, I
comment on three strategies to run simulations for which the values taken by two
arguments (or more) vary from one simulation to another.&lt;/p>
&lt;h2 id="the-problem">The problem&lt;/h2>
&lt;p>Let&amp;rsquo;s assume that we run simulations by calling a
&lt;a href="https://julialang.org/">Julia&lt;/a> script &lt;code>myscript.jl&lt;/code> that takes &lt;code>p1&lt;/code> as
argument, in order to run the simulation on one
&lt;a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU&lt;/a> for a single
simulation, I would run the following bas command&lt;/p>
&lt;div class="highlight">&lt;div style="color:#575279;background-color:#faf4ed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#575279;background-color:#faf4ed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#575279;background-color:#faf4ed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>julia -p &lt;span style="color:#ea9d34">1&lt;/span> myscript.jl p1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>where I would replace &lt;code>p1&lt;/code> by the right value. To run more than one simulation,
say 3 for the sake of example, with Slurm I would write a short bash script and use a &lt;a href="https://slurm.schedmd.com/job_array.html">job array&lt;/a> (see
more details in &lt;a href="../arrayjob">this note&lt;/a>) be it for simple sequence, e.g.&lt;code>1:3&lt;/code>,&lt;/p></description></item><item><title>Array job with Slurm: a use case and a mistake I made!</title><link>https://kevcaz.github.io/notes/2020/06/2020-06-19_array_job_with/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://kevcaz.github.io/notes/2020/06/2020-06-19_array_job_with/</guid><description>&lt;p>&amp;#x26a0;&amp;#xfe0f; I won&amp;rsquo;t introduce &lt;a href="https://slurm.schedmd.com">Slurm&lt;/a> here as I already have written several notes about it (see the &lt;a href="https://kevcaz.github.io/tags/slurm/">Slurm tag&lt;/a>).&lt;/p>
&lt;p>Today, I needed to send 171 jobs to the server. All jobs were similar similar,
only the species ID was changing, exactly the kind of computation &lt;a href="https://docs.computecanada.ca/wiki/Running_jobs#Array_job">array
jobs&lt;/a> were designed
for. As there are 32 cpus per nodes, I figured I needed 6 nodes. And so I wrote
the following bash script:&lt;/p>
&lt;div class="highlight">&lt;div style="color:#575279;background-color:#faf4ed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#575279;background-color:#faf4ed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#575279;background-color:#faf4ed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#9893a5">#!/bin/bash
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#9893a5">&lt;/span>&lt;span style="color:#9893a5">#SBATCH --account=def-someone&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#9893a5">#SBATCH --time=2:00:00&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#9893a5">#SBATCH --nodes=6nodes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#9893a5">#SBATCH --cpus-per-task=1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#9893a5">#SBATCH --mem-per-cpu=4G&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#9893a5">#SBATCH --array=1-171&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#9893a5">#SBATCH --mail-user=kcazelle@uoguelph.ca&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#9893a5">#SBATCH --mail-type=FAIL&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#d7827e">cd&lt;/span> &lt;span style="color:#d7827e">$SLURM_SUBMIT_DIR&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Rscript ./launch.R &lt;span style="color:#d7827e">$SLURM_ARRAY_TASK_ID&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>But this was wrong&lt;/strong> because with this request SLURM allocates 6 nodes for
every task whereas I needed 6 nodes total. My mistake was to not realize that
jobs in a array job share &lt;strong>all&lt;/strong> the &lt;code>#SBATCH&lt;/code> options (but the &lt;code>--array&lt;/code> one),
including the number of nodes. Now, what I have bear in my mind is that an array
job is a bunch of serial job with the same &lt;code>#SBATCH&lt;/code> options and SLURM will
handle cpus/nodes needed and dispatch jobs anywhere it can! This is way more
flexible than requesting nodes and this makes it quite powerful for the kind of
computation I do!&lt;/p></description></item><item><title>Submitting a Julia script with Slurm</title><link>https://kevcaz.github.io/notes/2020/05/2020-05-01_submitting_a_julia/</link><pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate><guid>https://kevcaz.github.io/notes/2020/05/2020-05-01_submitting_a_julia/</guid><description>&lt;p>In a &lt;a href="../../notes/computesci/graham/">previous note&lt;/a>, I&amp;rsquo;ve narrated my transition
from Mammoth to Graham and I&amp;rsquo;ve exemplified how to submit an &lt;a href="https://www.r-project.org/">&lt;i class="fab
fa-r-project">&lt;/i>&lt;/a> job with
&lt;a href="https://slurm.schedmd.com">Slurm&lt;/a>. As I&amp;rsquo;m currently using
&lt;a href="https://julialang.org/">Julia&lt;/a> for several projects, I&amp;rsquo;d like to report how I
submitted my Julia script to the scheduler.&lt;/p>
&lt;p>First of all, I needed to set up Julia for my account. As
&lt;a href="https://docs.computecanada.ca/wiki/Graham">Graham&lt;/a> runs under CentOS and as I
had already loaded Julia v1.3.1&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, I just had to update the version of
Julia with &lt;a href="https://linux.die.net/man/1/module">&lt;code>module&lt;/code>&lt;/a>.&lt;/p></description></item><item><title>Combining Slurm and GNU parallel</title><link>https://kevcaz.github.io/notes/2019/11/2019-11-11_combining_slurm_and/</link><pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate><guid>https://kevcaz.github.io/notes/2019/11/2019-11-11_combining_slurm_and/</guid><description>&lt;p>Last week, I attended a &amp;ldquo;Midi conférence&amp;rdquo; (basically a one hour training session
during lunch time) offered by Compute Canada dealing with
&lt;a href="https://slurm.schedmd.com/quickstart.html">Slurm&lt;/a>, &lt;a href="https://www.gnu.org/software/parallel/">GNU
Parallel&lt;/a> and how to combine them. This
was a very useful and timely presentation for me (&lt;a href="https://docs.google.com/presentation/d/1ysIaSWa157yiZ-ocX1jkAys1NkGgwolArSwCgAYCUPQ/edit?ts=5dc04169#slide=id.g65b5e056e3_0_5">&amp;#x1f1eb;&amp;#x1f1f7; slides available online
&amp;#x1f517;&lt;/a>)
as I&amp;rsquo;ve just got started with Slurm (see my &lt;a href="../../notes/computesci/graham">previous notes on the
topic&lt;/a>) and was eager to learn more.&lt;/p>
&lt;p>Earlier today, I found an opportunity to put in practice what I&amp;rsquo;ve learned last
week. Indeed I needed to download hundreds of shapefiles (2 different kind of
shapefiles at 2 different for almost 120 years), extract values from them before
deleting them. To do so, I wrote the following bash script to distribute the
simulations on 5 nodes and use 4 CPUs per node:&lt;/p></description></item><item><title>From Mammoth to Graham</title><link>https://kevcaz.github.io/notes/2019/09/2019-09-19_from_mammoth_to/</link><pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate><guid>https://kevcaz.github.io/notes/2019/09/2019-09-19_from_mammoth_to/</guid><description>&lt;p>&lt;a href="https://www.computecanada.ca/home/">Compute Canada&lt;/a> is awesome! Seriously, it is! As explained on its website:&lt;/p>
&lt;blockquote>
&lt;p>Compute Canada, in partnership with regional organizations ACENET, Calcul Québec, Compute Ontario and WestGrid, leads the acceleration of research and innovation by deploying state-of-the-art advanced research computing (ARC) systems, storage and software solutions. Together we provide essential ARC services and infrastructure for Canadian researchers and their collaborators in all academic and industrial sectors. Our world-class team of more than 200 experts employed by 37 partner universities and research institutions across the country provide direct support to research teams. Compute Canada is a proud ambassador for Canadian excellence in advanced research computing nationally and internationally.&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p></description></item></channel></rss>